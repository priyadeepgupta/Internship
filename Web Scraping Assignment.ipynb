{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46494603",
   "metadata": {},
   "source": [
    "#                      ASSIGNMENT-1\n",
    "                      \n",
    "                      WEB SCRAPING\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fd9492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\sanja\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a68783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008eae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de6f9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "page\n",
    "\n",
    "if page.status_code == 200:\n",
    "  \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "   \n",
    "    header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "    \n",
    "  \n",
    "    header_text = [tag.text for tag in header_tags]\n",
    "    \n",
    "   \n",
    "    df = pd.DataFrame({\"Headers\": header_text})\n",
    "    \n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be097f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5daaad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage.\n"
     ]
    }
   ],
   "source": [
    "#  2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "#from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "page = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "\n",
    "if page.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\", class_=\"table-bordered\")\n",
    "    \n",
    "    names = []\n",
    "    terms = []\n",
    "    \n",
    "    for row in table.find_all(\"tr\")[1:]:  \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 2:\n",
    "            name, term = [column.get_text(strip=True) for column in columns]\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame({\"Name\": names, \"Term of Office\": terms})\n",
    "    \n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c294f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d596dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team           Matches Points Rating\n",
      "0    1        India\\nIND     49    119\n",
      "1    2    Australia\\nAUS     36    112\n",
      "2    3     Pakistan\\nPAK     32    110\n",
      "3    4  South Africa\\nSA     29    109\n",
      "4    5   New Zealand\\nNZ     38    105\n",
      "5    6      England\\nENG     34     99\n",
      "6    7     Sri Lanka\\nSL     43     92\n",
      "7    8   Bangladesh\\nBAN     40     89\n",
      "8    9  Afghanistan\\nAFG     26     83\n",
      "9   10   West Indies\\nWI     38     68\n"
     ]
    }
   ],
   "source": [
    "#  Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n",
    "#  a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "\n",
    "if response.status_code == 200:\n",
    " \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    " \n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "  \n",
    "    for row in table.find_all(\"tr\")[1:11]: \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 5:\n",
    "            team, match, point, _, rating = [column.text.strip() for column in columns]\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({ \"Team\": teams, \"Matches\": matches,  \"Points\": points, \"Rating\": ratings })\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cab07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "   \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "   \n",
    "    for row in table.find_all(\"tr\")[1:11]:  \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 4:\n",
    "            player, team, rating = [column.text.strip() for column in columns]\n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({ \"Player\": players, \"Team\": teams, \"Rating\": ratings })\n",
    "\n",
    "   \n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df123a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#   c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "   \n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "   \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "  \n",
    "    for row in table.find_all(\"tr\")[1:11]:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 4:\n",
    "            player, team, rating = [column.text.strip() for column in columns]\n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({ \"Player\": players, \"Team\": teams, \"Rating\": ratings })\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597e8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faca1678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team           Matches Points Rating\n",
      "0    1    Australia\\nAUS     19    162\n",
      "1    2      England\\nENG     23    130\n",
      "2    3  South Africa\\nSA     21    116\n",
      "3    4        India\\nIND     18     97\n",
      "4    5   New Zealand\\nNZ     21     96\n",
      "5    6   West Indies\\nWI     18     89\n",
      "6    7     Sri Lanka\\nSL      9     79\n",
      "7    8   Bangladesh\\nBAN     11     74\n",
      "8    9     Thailand\\nTHA     11     68\n",
      "9   10     Pakistan\\nPAK     21     68\n"
     ]
    }
   ],
   "source": [
    "#  4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#  a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    " \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "  \n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:11]: \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 5:\n",
    "            team, match, point, _, rating = [column.text.strip() for column in columns]\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({ \"Team\": teams, \"Matches\": matches, \"Points\": points, \"Rating\": ratings })\n",
    "\n",
    " \n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d1c65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:11]:  \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 4:\n",
    "            player, team, rating = [column.text.strip() for column in columns]\n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({ \"Player\": players, \"Team\": teams, \"Rating\": ratings })\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12b1aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "  \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:11]: \n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 4:\n",
    "            player, team, rating = [column.text.strip() for column in columns]\n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({ \"Player\": players, \"Team\": teams, \"Rating\": ratings })\n",
    "    \n",
    "    print(df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09776c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a231fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "#  i) Headline\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    headlines = soup.find_all(\"h3\", class_=\"Card-title\")\n",
    "\n",
    "    headline_list = [headline.text.strip() for headline in headlines]\n",
    "\n",
    "    df = pd.DataFrame({\"Headline\": headline_list})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c2a9541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  ii) Time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    " \n",
    "    articles = soup.find_all(\"div\", class_=\"Card-details\")\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "\n",
    "    for article in articles:\n",
    "        headline = article.find(\"h3\", class_=\"Card-title\")\n",
    "        time = article.find(\"time\", class_=\"Card-time\")\n",
    "        \n",
    "        if headline and time:\n",
    "            headlines.append(headline.text.strip())\n",
    "            times.append(time.text.strip())\n",
    "\n",
    "    df = pd.DataFrame({\"Headline\": headlines, \"Time\": times})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93834aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  iii) News Link\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"Card-details\")\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    links = []\n",
    "\n",
    "    for article in articles:\n",
    "        headline = article.find(\"h3\", class_=\"Card-title\")\n",
    "        time = article.find(\"time\", class_=\"Card-time\")\n",
    "        link = article.find(\"a\", class_=\"Card-heading\")\n",
    "\n",
    "        if headline and time and link:\n",
    "            headlines.append(headline.text.strip())\n",
    "            times.append(time.text.strip())\n",
    "            links.append(\"https://www.cnbc.com\" + link['href'])\n",
    "\n",
    "    df = pd.DataFrame({\"Headline\": headlines, \"Time\": times, \"Link\": links})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901addb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c740eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#  Scrape below mentioned details and make data frame\n",
    "#  i) Paper Title\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "  \n",
    "    paper_titles = soup.find_all(\"h2\", class_=\"articles__title\")\n",
    "\n",
    "    paper_title_list = [title.text.strip() for title in paper_titles]\n",
    "\n",
    "    df = pd.DataFrame({\"Paper Title\": paper_title_list})\n",
    "\n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "566e04c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  ii) Authors\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "if response.status_code == 200:\n",
    " \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paper_info = soup.find_all(\"div\", class_=\"pod-listing__header\")\n",
    "\n",
    "    paper_titles = []\n",
    "    authors = []\n",
    "\n",
    "    for info in paper_info:\n",
    "        title = info.find(\"h2\", class_=\"articles__title\")\n",
    "        author = info.find(\"div\", class_=\"text-m authors__authors\")\n",
    "        if title and author:\n",
    "            paper_titles.append(title.text.strip())\n",
    "            authors.append(author.text.strip())\n",
    "\n",
    "    df = pd.DataFrame({\"Paper Title\": paper_titles, \"Authors\": authors})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08c010b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  iii) Published Date\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "if response.status_code == 200:\n",
    " \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paper_info = soup.find_all(\"div\", class_=\"pod-listing__header\")\n",
    "    date_info = soup.find_all(\"div\", class_=\"text-m date__published\")\n",
    "\n",
    "    paper_titles = []\n",
    "    authors = []\n",
    "    published_dates = []\n",
    "\n",
    "    for info in paper_info:\n",
    "        title = info.find(\"h2\", class_=\"articles__title\")\n",
    "        author = info.find(\"div\", class_=\"text-m authors__authors\")\n",
    "        if title:\n",
    "            paper_titles.append(title.text.strip())\n",
    "        if author:\n",
    "            authors.append(author.text.strip())\n",
    "\n",
    "    for date in date_info:\n",
    "        published_dates.append(date.text.strip())\n",
    "\n",
    "    df = pd.DataFrame({\"Paper Title\": paper_titles, \"Authors\": authors, \"Published Date\": published_dates})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f985d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "  \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "  \n",
    "    paper_info = soup.find_all(\"div\", class_=\"pod-listing__header\")\n",
    "    date_info = soup.find_all(\"div\", class_=\"text-m date__published\")\n",
    "\n",
    "    paper_titles = []\n",
    "    authors = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    for info in paper_info:\n",
    "        title = info.find(\"h2\", class_=\"articles__title\")\n",
    "        author = info.find(\"div\", class_=\"text-m authors__authors\")\n",
    "        link = info.find(\"a\", class_=\"pod-listing__button\")\n",
    "        if title:\n",
    "            paper_titles.append(title.text.strip())\n",
    "        if author:\n",
    "            authors.append(author.text.strip())\n",
    "        if link:\n",
    "            paper_urls.append(link['href'])\n",
    "\n",
    "    for date in date_info:\n",
    "        published_dates.append(date.text.strip())\n",
    "\n",
    "    df = pd.DataFrame({\"Paper Title\": paper_titles, \"Authors\": authors, \"Published Date\": published_dates, \"Paper URL\": paper_urls})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d37695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8da928b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Castle BarbequeConnaught Place, Central Delhi',\n",
       " 'Cafe KnoshThe Leela Ambience Convention Hotel,Shahdara, East Delhi',\n",
       " 'India GrillHilton Garden Inn,Saket, South Delhi',\n",
       " 'The Barbeque CompanyGardens Galleria,Sector 38A, Noida',\n",
       " 'Delhi BarbequeTaurus Sarovar Portico,Mahipalpur, South Delhi',\n",
       " 'The Monarch - Bar Be Que VillageIndirapuram Habitat Centre,Indirapuram, Ghaziabad',\n",
       " 'Indian Grill RoomSuncity Business Tower,Golf Course Road, Gurgaon',\n",
       " 'The Barbeque TimesM2K Corporate Park,Sector 51, Gurgaon']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "# i) Restaurant name\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "page = requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "    \n",
    "first_title = soup.find('div', class_=\"restnt-info cursor\")\n",
    "first_title\n",
    "\n",
    "first_title.text\n",
    "\n",
    "titles = []\n",
    "\n",
    "for i in soup.find_all('div', class_=\"restnt-info cursor\"):\n",
    "    titles.append(i.text)\n",
    "    \n",
    "    \n",
    "titles\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5565191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Cuisine]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  ii) Cuisine\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.dineout.co.in/delhi-restaurants\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    cuisine_elements = soup.find_all(\"div\", class_=\"restnt-info-main\")\n",
    "\n",
    "    cuisine_list = []\n",
    "\n",
    "\n",
    "    for element in cuisine_elements:\n",
    "        cuisine_div = element.find(\"div\", class_=\"restnt-info\")\n",
    "        cuisine_span = cuisine_div.find(\"span\", class_=\"double-line-ellipsis\")\n",
    "        if cuisine_span:\n",
    "            cuisine = cuisine_span.text.strip()\n",
    "            cuisine_list.append(cuisine)\n",
    "\n",
    "    df = pd.DataFrame({\"Cuisine\": cuisine_list})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b28ef638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Connaught Place, Central Delhi',\n",
       " 'M-Block,Connaught Place, Central Delhi',\n",
       " 'M-Block,Connaught Place, Central Delhi',\n",
       " 'F-Block,Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Regal Cinema Complex,Connaught Place, Central Delhi',\n",
       " 'Scindia House,Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'The Colonnade,Connaught Place, Central Delhi',\n",
       " 'Shangri La Eros New Delhi,Janpath, Central Delhi',\n",
       " 'Connaught Place, Central Delhi',\n",
       " 'F-Block,Connaught Place, Central Delhi',\n",
       " 'Connaught Place, Central Delhi']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  iii) Location\n",
    "\n",
    "location = soup.find('div',class_=\"restnt-loc ellipsis\")\n",
    "location\n",
    "location.text\n",
    "\n",
    "location = []\n",
    "\n",
    "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "    \n",
    "\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8b49f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Ratings]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#  iv) Ratings\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://www.dineout.co.in/delhi-restaurants\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    rating_elements = soup.find_all(\"div\", class_=\"restnt-info-main\")\n",
    "\n",
    "    rating_list = []\n",
    "\n",
    "    for element in rating_elements:\n",
    "        rating_div = element.find(\"div\", class_=\"restnt-info\")\n",
    "        rating_span = rating_div.find(\"span\", class_=\"rating\")\n",
    "        if rating_span:\n",
    "            rating = rating_span.text.strip()\n",
    "            rating_list.append(rating)\n",
    "\n",
    "    df = pd.DataFrame({\"Ratings\": rating_list})\n",
    "\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d34dcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/c/z/p77626-15833232035e5f98437554f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/a/d/p32381-1495943585592a49a166fe7.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/m/u/p31393-15972091555f337a43bb961.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/9/l/q/p971-16946056676501a163bf712.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/d/f/p19636-16463919576221f295ae1f1.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/c/o/p126736-169693761665253690bc6fb.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/s/y/p20298-1664186585633178d99f1db.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/m/i/p12709-15681189525d7798a8c5aa5.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/h/o/p420-15241335515ad86eaf373ce.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/e/u/p80493-16064603115fc0a397716de.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/a/j/p20996-145631488756cd9a0796608.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/z/l/p2898-15451127095c188c853d291.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/b/t/p27452-15020105505986dcb6d147f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/e/y/p20941-168915645564ae7b673b872.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/e/q/p19748-169217151564dc7cfba4753.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/y/m/p221-16455303976214cd1d7a086.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/c/p3007-15706949095d9ee6fd0b8b4.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/y/w/p19271-169043635864c2030646158.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/y/y/p4199-164188203361dd21b1a3174.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/s/q/p21753-162678610360f6c93732f2f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/9/x/t/p98423-1634644819616eb353a1181.jpg?tr=tr:n-medium']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  v) Image URL\n",
    "\n",
    "images = []\n",
    "\n",
    "for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04161fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
