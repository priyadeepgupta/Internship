{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34eb9ed",
   "metadata": {},
   "source": [
    "#                            ASSIGNMENT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4daedcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\sanja\\anaconda3\\lib\\site-packages (4.15.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from selenium) (0.23.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: outcome in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "#= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "#A)Rank\n",
    "#B) Name\n",
    "#C) Artist\n",
    "#D) Upload date\n",
    "#E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9804fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Extract data from each column\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     49\u001b[0m name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     50\u001b[0m artist \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]: \n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip()\n",
    "    \n",
    "\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}, Name: {name_list[i]}, Artist: {artist_list[i]}, Upload Date: {upload_date_list[i]}, Views: {views_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "#Url = https://www.bcci.tv/.\n",
    "#You need to find following details:\n",
    "#A) Series\n",
    "#B) Place\n",
    "#C) Date\n",
    "#D) Time\n",
    "#Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f69007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7797C4D02+56194]\n",
      "\t(No symbol) [0x00007FF7797304B2]\n",
      "\t(No symbol) [0x00007FF7795D76AA]\n",
      "\t(No symbol) [0x00007FF7796216D0]\n",
      "\t(No symbol) [0x00007FF7796217EC]\n",
      "\t(No symbol) [0x00007FF779664D77]\n",
      "\t(No symbol) [0x00007FF779645EBF]\n",
      "\t(No symbol) [0x00007FF779662786]\n",
      "\t(No symbol) [0x00007FF779645C23]\n",
      "\t(No symbol) [0x00007FF779614A45]\n",
      "\t(No symbol) [0x00007FF779615AD4]\n",
      "\tGetHandleVerifier [0x00007FF779B3D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF779B96197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF779B8DF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF77985F029+687785]\n",
      "\t(No symbol) [0x00007FF77973B508]\n",
      "\t(No symbol) [0x00007FF779737564]\n",
      "\t(No symbol) [0x00007FF7797376E9]\n",
      "\t(No symbol) [0x00007FF779728094]\n",
      "\tBaseThreadInitThunk [0x00007FFDBD0F257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFDBD46AA58+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "try:\n",
    "    international_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//li[@data-nav-target=\"#international\"]'))\n",
    "    )\n",
    "    international_link.click()\n",
    "\n",
    "    fixtures_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//li[@data-nav-target=\"#international/fixtures\"]'))\n",
    "    )\n",
    "    fixtures_link.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    fixture_container = soup.find('div', {'class': 'js-list'})\n",
    "\n",
    "    series_list = []\n",
    "    place_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    for fixture_item in fixture_container.find_all('li', {'class': 'fixture__item'}):\n",
    "        series = fixture_item.find('span', {'class': 'u-unskewed-text'}).text.strip()\n",
    "        place = fixture_item.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "        date = fixture_item.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "        time = fixture_item.find('span', {'class': 'fixture__time'}).text.strip()\n",
    "\n",
    "        series_list.append(series)\n",
    "        place_list.append(place)\n",
    "        date_list.append(date)\n",
    "        time_list.append(time)\n",
    "\n",
    "    for i in range(len(series_list)):\n",
    "        print(f\"Series: {series_list[i]}, Place: {place_list[i]}, Date: {date_list[i]}, Time: {time_list[i]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23798c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1019ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Scrape the details of State-wise GDP of India from statisticstime.com.Url = http://statisticstimes.com/\n",
    "#You have to find following details: \n",
    "#A) Rank\n",
    "#B) State\n",
    "#C) GSDP(18-19)- at current prices\n",
    "#D) GSDP(19-20)- at current prices\n",
    "#E) Share(18-19)\n",
    "#F) GDP($ billion)\n",
    "#Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea6cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: element not interactable\n",
      "  (Session info: chrome=120.0.6099.71)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7797C4D02+56194]\n",
      "\t(No symbol) [0x00007FF7797304B2]\n",
      "\t(No symbol) [0x00007FF7795D74C3]\n",
      "\t(No symbol) [0x00007FF779622B99]\n",
      "\t(No symbol) [0x00007FF77961687F]\n",
      "\t(No symbol) [0x00007FF779645E7A]\n",
      "\t(No symbol) [0x00007FF779616226]\n",
      "\t(No symbol) [0x00007FF779646320]\n",
      "\t(No symbol) [0x00007FF779662786]\n",
      "\t(No symbol) [0x00007FF779645C23]\n",
      "\t(No symbol) [0x00007FF779614A45]\n",
      "\t(No symbol) [0x00007FF779615AD4]\n",
      "\tGetHandleVerifier [0x00007FF779B3D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF779B96197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF779B8DF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF77985F029+687785]\n",
      "\t(No symbol) [0x00007FF77973B508]\n",
      "\t(No symbol) [0x00007FF779737564]\n",
      "\t(No symbol) [0x00007FF7797376E9]\n",
      "\t(No symbol) [0x00007FF779728094]\n",
      "\tBaseThreadInitThunk [0x00007FFDBD0F257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFDBD46AA58+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "try:\n",
    "    economy_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//div[@class=\"navbar\"]/div[2]/div/a[3]'))\n",
    "    )\n",
    "    economy_link.click()\n",
    "\n",
    "    gdp_states_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'GDP of Indian states'))\n",
    "    )\n",
    "    gdp_states_link.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    gdp_table = soup.find('table', {'class': 'display dataTable'})\n",
    "\n",
    "    rank_list = []\n",
    "    state_list = []\n",
    "    gsdp_18_19_list = []\n",
    "    gsdp_19_20_list = []\n",
    "    share_18_19_list = []\n",
    "    gdp_billion_list = []\n",
    "\n",
    "    for row in gdp_table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "    \n",
    "        rank = columns[0].text.strip()\n",
    "        state = columns[1].text.strip()\n",
    "        gsdp_18_19 = columns[2].text.strip()\n",
    "        gsdp_19_20 = columns[3].text.strip()\n",
    "        share_18_19 = columns[4].text.strip()\n",
    "        gdp_billion = columns[5].text.strip()\n",
    "\n",
    "        rank_list.append(rank)\n",
    "        state_list.append(state)\n",
    "        gsdp_18_19_list.append(gsdp_18_19)\n",
    "        gsdp_19_20_list.append(gsdp_19_20)\n",
    "        share_18_19_list.append(share_18_19)\n",
    "        gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "    for i in range(len(rank_list)):\n",
    "        print(f\"Rank: {rank_list[i]}, State: {state_list[i]}, GSDP(18-19): {gsdp_18_19_list[i]}, \"\n",
    "              f\"GSDP(19-20): {gsdp_19_20_list[i]}, Share(18-19): {share_18_19_list[i]}, GDP($ billion): {gdp_billion_list[i]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6729ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Scrape the details of trending repositories on Github.com.Url = https://github.com/\n",
    "#You have to find the following details:\n",
    "#A) Repository title\n",
    "#B) Repository description\n",
    "#C) Contributors count\n",
    "#D) Language used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://github.com/\")\n",
    "\n",
    "try:\n",
    "    explore_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//nav[@class=\"mr-0 mr-lg-3 flex-order-1 flex-lg-order-none mt-3 mt-lg-0\"]/a[4]'))\n",
    "    )\n",
    "    explore_link.click()\n",
    "\n",
    "    trending_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, 'Trending'))\n",
    "    )\n",
    "    trending_link.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    repo_container = soup.find('ol', {'class': 'repo-list'})\n",
    "\n",
    "    title_list = []\n",
    "    description_list = []\n",
    "    contributors_count_list = []\n",
    "    language_list = []\n",
    "\n",
    "    for repo_item in repo_container.find_all('li', {'class': 'col-12 d-flex width-full py-4 border-bottom color-border-secondary public source'}):\n",
    "        title = repo_item.find('a', {'class': 'v-align-middle'}).text.strip()\n",
    "        description = repo_item.find('p', {'class': 'col-9 color-text-secondary my-1 pr-4'}).text.strip()\n",
    "        contributors_count = repo_item.find('a', {'class': 'muted-link d-inline-block mr-3'}).text.strip()\n",
    "        language = repo_item.find('span', {'itemprop': 'programmingLanguage'}).text.strip() if repo_item.find('span', {'itemprop': 'programmingLanguage'}) else \"Not specified\"\n",
    "\n",
    "        title_list.append(title)\n",
    "        description_list.append(description)\n",
    "        contributors_count_list.append(contributors_count)\n",
    "        language_list.append(language)\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        print(f\"Repository Title: {title_list[i]}\\nDescription: {description_list[i]}\\n\"\n",
    "              f\"Contributors Count: {contributors_count_list[i]}\\nLanguage Used: {language_list[i]}\\n{'='*50}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd389b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find thefollowing details:\n",
    "#A) Song name\n",
    "#B) Artist name\n",
    "#C) Last week rank\n",
    "#D) Peak rank\n",
    "#E) Weeks on board\n",
    "# Note: - From the home page you have to click on the charts option then hot 100-page link through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ea3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "\n",
    "try:\n",
    "    charts_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//li[@class=\"header__main-link header__main-link--charts \"]/a'))\n",
    "    )\n",
    "    charts_link.click()\n",
    "\n",
    "    hot_100_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//a[@href=\"/charts/hot-100/\"]'))\n",
    "    )\n",
    "    hot_100_link.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    songs_container = soup.find('ol', {'class': 'chart-list__elements'})\n",
    "\n",
    "    song_name_list = []\n",
    "    artist_name_list = []\n",
    "    last_week_rank_list = []\n",
    "    peak_rank_list = []\n",
    "    weeks_on_board_list = []\n",
    "\n",
    "    for song_item in songs_container.find_all('li', {'class': 'chart-list__element'}):\n",
    "        song_name = song_item.find('span', {'class': 'chart-element__information__song text--truncate color--primary'}).text.strip()\n",
    "        artist_name = song_item.find('span', {'class': 'chart-element__information__artist text--truncate color--secondary'}).text.strip()\n",
    "        last_week_rank = song_item.find('span', {'class': 'chart-element__meta text--center color--secondary text--last'}).text.strip()\n",
    "        peak_rank = song_item.find('span', {'class': 'chart-element__meta text--center color--secondary text--peak'}).text.strip()\n",
    "        weeks_on_board = song_item.find('span', {'class': 'chart-element__meta text--center color--secondary text--week'}).text.strip()\n",
    "\n",
    "        song_name_list.append(song_name)\n",
    "        artist_name_list.append(artist_name)\n",
    "        last_week_rank_list.append(last_week_rank)\n",
    "        peak_rank_list.append(peak_rank)\n",
    "        weeks_on_board_list.append(weeks_on_board)\n",
    "\n",
    "    for i in range(len(song_name_list)):\n",
    "        print(f\"Song Name: {song_name_list[i]}\\nArtist Name: {artist_name_list[i]}\\n\"\n",
    "              f\"Last Week Rank: {last_week_rank_list[i]}\\nPeak Rank: {peak_rank_list[i]}\\n\"\n",
    "              f\"Weeks on Board: {weeks_on_board_list[i]}\\n{'='*50}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dd181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5757ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Scrape the details of Highest selling novels.\n",
    "#A) Book name\n",
    "#B) Author name\n",
    "#C) Volumes sold\n",
    "#D) Publisher\n",
    "#E) Genre\n",
    " #Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'in-article sortable'})\n",
    "\n",
    "book_name_list = []\n",
    "author_name_list = []\n",
    "volumes_sold_list = []\n",
    "publisher_list = []\n",
    "genre_list = []\n",
    "\n",
    "for row in table.find_all('tr')[1:]:  \n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    book_name_list.append(book_name)\n",
    "    author_name_list.append(author_name)\n",
    "    volumes_sold_list.append(volumes_sold)\n",
    "    publisher_list.append(publisher)\n",
    "    genre_list.append(genre)\n",
    "\n",
    "for i in range(len(book_name_list)):\n",
    "    print(f\"Book Name: {book_name_list[i]}\\nAuthor Name: {author_name_list[i]}\\n\"\n",
    "          f\"Volumes Sold: {volumes_sold_list[i]}\\nPublisher: {publisher_list[i]}\\nGenre: {genre_list[i]}\\n{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdbf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aedc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'lister-item-content')))\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "tv_series_container = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "name_list = []\n",
    "year_span_list = []\n",
    "genre_list = []\n",
    "run_time_list = []\n",
    "ratings_list = []\n",
    "votes_list = []\n",
    "\n",
    "for series_item in tv_series_container:\n",
    "    name = series_item.find('h3', class_='lister-item-header').a.text.strip()\n",
    "    year_span = series_item.find('span', class_='lister-item-year').text.strip('()')\n",
    "    genre = series_item.find('span', class_='genre').text.strip()\n",
    "    run_time = series_item.find('span', class_='runtime').text.strip()\n",
    "    ratings = series_item.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    votes = series_item.find('span', {'name': 'nv'}).text.strip()\n",
    "\n",
    "    name_list.append(name)\n",
    "    year_span_list.append(year_span)\n",
    "    genre_list.append(genre)\n",
    "    run_time_list.append(run_time)\n",
    "    ratings_list.append(ratings)\n",
    "    votes_list.append(votes)\n",
    "\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"Name: {name_list[i]}\\nYear Span: {year_span_list[i]}\\nGenre: {genre_list[i]}\\n\"\n",
    "          f\"Run Time: {run_time_list[i]}\\nRatings: {ratings_list[i]}\\nVotes: {votes_list[i]}\\n{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a6811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Details of Datasets from UCI machine learning repositories.\n",
    "#Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "#A) Dataset name\n",
    "#B) Data type\n",
    "#C) Task\n",
    "#D) Attribute type\n",
    "#E) No of instances\n",
    "#F) No of attribute G) Year\n",
    "# Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed38f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "\n",
    "try:\n",
    "    view_all_link = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//span[@class=\"normal\"]/b/a'))\n",
    "    )\n",
    "    view_all_link.click()\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    dataset_table = soup.find('table', {'border': '1'})\n",
    "\n",
    "    dataset_name_list = []\n",
    "    data_type_list = []\n",
    "    task_list = []\n",
    "    attribute_type_list = []\n",
    "    no_of_instances_list = []\n",
    "    no_of_attributes_list = []\n",
    "    year_list = []\n",
    "\n",
    "    for row in dataset_table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "\n",
    "        dataset_name = columns[0].text.strip()\n",
    "        data_type = columns[1].text.strip()\n",
    "        task = columns[2].text.strip()\n",
    "        attribute_type = columns[3].text.strip()\n",
    "        no_of_instances = columns[4].text.strip()\n",
    "        no_of_attributes = columns[5].text.strip()\n",
    "        year = columns[6].text.strip()\n",
    "\n",
    "        dataset_name_list.append(dataset_name)\n",
    "        data_type_list.append(data_type)\n",
    "        task_list.append(task)\n",
    "        attribute_type_list.append(attribute_type)\n",
    "        no_of_instances_list.append(no_of_instances)\n",
    "        no_of_attributes_list.append(no_of_attributes)\n",
    "        year_list.append(year)\n",
    "\n",
    "    for i in range(len(dataset_name_list)):\n",
    "        print(f\"Dataset Name: {dataset_name_list[i]}\\nData Type: {data_type_list[i]}\\nTask: {task_list[i]}\\n\"\n",
    "              f\"Attribute Type: {attribute_type_list[i]}\\nNo of Instances: {no_of_instances_list[i]}\\n\"\n",
    "              f\"No of Attributes: {no_of_attributes_list[i]}\\nYear: {year_list[i]}\\n{'='*50}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
